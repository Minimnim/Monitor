{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the terminal: pip install torch torchvision torchaudio \n",
    "#in the terminal: pip install pytorch-lightning \n",
    "#in the terminal: pip install pandas \n",
    "#in the terminal: pip install pytorch-lightning pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu121/torch_stable.html\n",
    "#in the terminal: pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\120224294\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from torchmetrics.functional import accuracy\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import ConcatDataset\n",
    "import random\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.metrics import roc_auc_score, f1_score, matthews_corrcoef\n",
    "import itertools\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('nirs with HIE outcome for CNN_10hours.csv', header = None)\n",
    "# label = pd.read_csv('HIE outcome for CNN_10hours.csv', header = None)\n",
    "# signals = df.values\n",
    "# signals = signals[180:,1:]\n",
    "# print(signals.shape)\n",
    "# # mean = np.mean(signals)\n",
    "# # std = np.std(signals)\n",
    "# # signals = (signals - mean)/std\n",
    "# label = label.values\n",
    "# label = label[180:,1]\n",
    "# label = label.reshape(-1,1)\n",
    "# # label = torch.tensor(label)\n",
    "# print(label.shape)\n",
    "# dataset = list(zip(signals, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_test_val_ids(df, n):\n",
    "    IDs = df[0].unique()\n",
    "    random_ids = pd.Series(IDs).sample(n , replace= False)\n",
    "    return random_ids\n",
    "\n",
    "def making_train_test_val_signals(df, label, hie, n):\n",
    "    a = making_test_val_ids(df, n)\n",
    "    train_signal = df[~df[0].isin(a)]\n",
    "    train_label = label[~label[0].isin(a)]\n",
    "    test_IDs = a[:(n//2)]\n",
    "    val_IDs = a[(n//2):]\n",
    "    # test_IDs = [22,15,12,21]\n",
    "    # val_IDs = [49,37,55,43]\n",
    "    train_signal = train_signal.values\n",
    "    train_signal = train_signal[:,1:]\n",
    "    train_hie = hie[~hie[0].isin(a)]\n",
    "    train_hie = train_hie.values\n",
    "    train_hie = train_hie[:,1]\n",
    "    train_label = train_label.values\n",
    "    train_label = train_label[:,1]\n",
    "    test_signal = df[df[0].isin(test_IDs)]\n",
    "    test_signal = test_signal.values\n",
    "    test_signal = test_signal[:,1:]\n",
    "    test_label = label[label[0].isin(test_IDs)]\n",
    "    test_label = test_label.values\n",
    "    test_label = test_label[:,1]\n",
    "    test_hie = hie[hie[0].isin(test_IDs)]\n",
    "    test_hie = test_hie.values\n",
    "    test_hie = test_hie[:,1]\n",
    "    val_signal = df[df[0].isin(val_IDs)]\n",
    "    val_signal = val_signal.values\n",
    "    val_signal = val_signal[:,1:]\n",
    "    val_label = label[label[0].isin(val_IDs)]\n",
    "    val_label = val_label.values\n",
    "    val_label = val_label[:,1]\n",
    "    val_hie = hie[hie[0].isin(val_IDs)]\n",
    "    val_hie = val_hie.values\n",
    "    val_hie = val_hie[:,1]\n",
    "    print(test_IDs)\n",
    "    print(val_IDs)\n",
    "    return train_signal, train_label, train_hie, test_signal, test_label, test_hie,  val_signal, val_label, val_hie\n",
    "\n",
    "def separate_epochs(train_signal, epoch_length):\n",
    "    num_epochs = 5890 // epoch_length\n",
    "    separated_epochs = np.split(train_signal, num_epochs, axis=1)\n",
    "    return separated_epochs\n",
    "\n",
    "def shuffle_epochs(epochs):\n",
    "    np.random.shuffle(epochs)\n",
    "    return epochs\n",
    "\n",
    "def reconstruct_signals(epochs):\n",
    "    reconstructed_signals = np.concatenate(epochs, axis=1)\n",
    "    return reconstructed_signals\n",
    "\n",
    "def separate_pieces(signal, num_pieces):\n",
    "    piece_length = 5890 // num_pieces\n",
    "    pieces = np.split(signal, num_pieces, axis=1)\n",
    "    return pieces\n",
    "\n",
    "\n",
    "\n",
    "def generate_new_signals(dataset, num_signals, num_rep, epoch_length, num):\n",
    "    filtered_dataset = [(train_signal, train_hie, train_label) for train_signal, train_hie,  train_label in dataset if train_label == num and train_hie == num]\n",
    "    filtered_signals = np.array([train_signal for train_signal, _ , _ in filtered_dataset])\n",
    "    filtered_signals = filtered_signals.reshape(-1, 1, 5890)\n",
    "    combined_signals = []\n",
    "    for _ in range(num_rep):\n",
    "        selected_indices = np.random.choice(filtered_signals.shape[0], size=num_signals, replace=False)\n",
    "        selected_signals = filtered_signals[selected_indices]\n",
    "\n",
    "        combined_epochs = []\n",
    "        for train_signal in selected_signals:\n",
    "            epochs = separate_epochs(train_signal, epoch_length)\n",
    "            combined_epochs += epochs\n",
    "\n",
    "        shuffled_epochs = shuffle_epochs(combined_epochs)\n",
    "        reconstructed_signals = reconstruct_signals(shuffled_epochs)\n",
    "        combined_signals.append(reconstructed_signals)\n",
    "\n",
    "    combined_signals = np.array(combined_signals)\n",
    "    # combined_signals = combined_signals.reshape(num_rep, 1, 5890)\n",
    "    combined_signals = combined_signals.reshape(num_rep, -1)[ :, :5890]\n",
    "    return combined_signals\n",
    "\n",
    "def generate_new_signals_from_one(dataset, num_pieces):\n",
    "    new_signals = []\n",
    "    for signal, hie, label in dataset:\n",
    "        signal = signal.reshape(1, 5890)\n",
    "        pieces = separate_pieces(signal, num_pieces)\n",
    "        permutations = list(itertools.permutations(pieces))\n",
    "        for perm in permutations:\n",
    "            # shuffled_pieces = shuffle_epochs(perm)\n",
    "            new_signal = np.concatenate(perm, axis=1)\n",
    "            new_signals.append((new_signal, hie, label))\n",
    "    return new_signals\n",
    "\n",
    "\n",
    "def generate_new_rows_amin(row, num_new_rows, window_percentage):\n",
    "    new_rows = []\n",
    "    for _ in range(num_new_rows):\n",
    "        row_length = len(row)\n",
    "        window_size = int(row_length * window_percentage)\n",
    "\n",
    "        # Randomly select start indices for the windows\n",
    "        start_idx_orig = random.randint(1, row_length - window_size)\n",
    "        start_idx_swap = random.randint(1, row_length - window_size)\n",
    "\n",
    "        # Generate the new row by swapping the windows\n",
    "        new_row = row[:]\n",
    "        new_row[start_idx_orig:start_idx_orig + window_size], new_row[start_idx_swap:start_idx_swap + window_size] = \\\n",
    "            new_row[start_idx_swap:start_idx_swap + window_size], new_row[start_idx_orig:start_idx_orig + window_size]\n",
    "\n",
    "        new_rows.append(new_row)\n",
    "    return new_rows\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('nirs with HIE outcome for CNN_10hours.csv', header = None)\n",
    "# label = pd.read_csv('HIE outcome for CNN_10hours.csv', header = None)\n",
    "# train_signal, train_label, test_signal, test_label, val_signal, val_label = making_train_test_val_signals(df, label, 10)\n",
    "# val_label = val_label.reshape(-1,1)\n",
    "# val_signal = val_signal.reshape(-1,1,5890)\n",
    "# test_label = test_label.reshape(-1,1)\n",
    "# test_signal = test_signal.reshape(-1,1,5890)\n",
    "# dataset = list(zip(train_signal, train_label))\n",
    "# val_dataset = list(zip(val_signal, val_label))\n",
    "# test_dataset = list(zip(test_signal, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dict = {}\n",
    "# for i in range(10):\n",
    "#     if i <4:\n",
    "#         train_label = 1\n",
    "#         filename = f'combined_signals_train_label_1_{i}.csv'\n",
    "#     else:\n",
    "#         train_label = 0\n",
    "#         filename = f'combined_signals_train_label_0_{i-4}.csv'\n",
    "\n",
    "#     df_combined_signals = generate_new_signals(dataset,150, 1000, 589)\n",
    "#     df_combined_signals = pd.DataFrame(df_combined_signals)\n",
    "#     # df_dict[filename] = filename\n",
    "#     df_combined_signals.to_csv(filename, header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_00 = pd.read_csv('combined_signals_train_label_0_0.csv', header = None)\n",
    "# df_01 = pd.read_csv('combined_signals_train_label_0_1.csv', header = None)\n",
    "# df_02 = pd.read_csv('combined_signals_train_label_0_2.csv', header = None)\n",
    "# df_03 = pd.read_csv('combined_signals_train_label_0_3.csv', header = None)\n",
    "# df_04 = pd.read_csv('combined_signals_train_label_0_4.csv', header = None)\n",
    "# df_05 = pd.read_csv('combined_signals_train_label_0_5.csv', header = None)\n",
    "# df_10 = pd.read_csv('combined_signals_train_label_1_0.csv', header = None)\n",
    "# df_11 = pd.read_csv('combined_signals_train_label_1_1.csv', header = None)\n",
    "# df_12 = pd.read_csv('combined_signals_train_label_1_2.csv', header = None)\n",
    "# df_13 = pd.read_csv('combined_signals_train_label_1_3.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_signals = np.concatenate([train_signal, df_00, df_01, df_02, df_03,\n",
    "#                                    df_04, df_05, df_10, df_11, df_12, df_13], axis=0)\n",
    "# arr0 = np.zeros((6000,1))\n",
    "# arr1 = np.ones((4000,1))\n",
    "# train_label = train_label.reshape(-1,1)\n",
    "# combined_label = np.concatenate([train_label, arr0, arr1], axis = 0)\n",
    "# combined_label = combined_label.reshape(-1,1)\n",
    "# combined_signals = combined_signals.reshape(-1,1,5890)\n",
    "# combined_signals = torch.tensor(combined_signals)\n",
    "# combined_label = torch.tensor(combined_label)\n",
    "# dataset = list(zip(combined_signals, combined_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set1 = set(tuple(signal.flatten()) for signal in signals)\n",
    "# set2 = set(tuple(signal.flatten()) for signal in combined_signals)\n",
    "\n",
    "# # Check if there are any same signals between the two groups\n",
    "# common_signals = set1.intersection(set2)\n",
    "\n",
    "# # Print the common signals, if any\n",
    "# if common_signals:\n",
    "#     print(\"Common signals found:\")\n",
    "#     for signal in common_signals:\n",
    "#         print(signal)\n",
    "# else:\n",
    "#     print(\"No common signals found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_set = set(tuple(signal.flatten()) for signal in combined_signals)\n",
    "\n",
    "# # Check if there are any duplicate signals in the dataset\n",
    "# if len(signal_set) < len(combined_signals):\n",
    "#     print(\"Duplicate signals found.\")\n",
    "# else:\n",
    "#     print(\"No duplicate signals found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.tensor([0.679]).cuda()\n",
    "# weights = torch.tensor([0.22]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# def add_noise(x):\n",
    "#     noise = torch.rand_like(x)*0.1\n",
    "#     return x + noise \n",
    "\n",
    "# def time_shift(x):\n",
    "#     shift = np.random.randint(-10,10)\n",
    "#     return torch.roll(x, shift, dims= 1)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Lambda(lambda x: x.unsqueeze(0)), \n",
    "#     transforms.Lambda(add_noise),\n",
    "#     transforms.Lambda(time_shift),\n",
    "#     transforms.Lambda(lambda x: x.unsqueeze(0))])\n",
    "\n",
    "# augmented_x = transform(signals)\n",
    "#augmented_dataset = list(zip(augmented_x, label))\n",
    "# combined_dataset_train = ConcatDataset([dataset[180:], augmented_x[180:]])\n",
    "\n",
    "# combined_dataset_train = ConcatDataset([dataset[180:] , df_0 , df_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # self.layer1 = nn.Sequential(\n",
    "    #   nn.Conv1d(1, 64, kernel_size= 10, stride=1, padding=2),\n",
    "    #   nn.LeakyReLU(),\n",
    "    #   nn.MaxPool1d(kernel_size=2, stride=6))\n",
    "    self.layer1 = nn.Sequential(\n",
    "      nn.Conv1d(1, 16, kernel_size= 11, stride=2, padding=0),\n",
    "      nn.BatchNorm1d(16),\n",
    "      nn.LeakyReLU())\n",
    "    # self.layer2 = nn.Sequential(\n",
    "    #   nn.Conv1d(64, 64, kernel_size= 10, stride=1, padding=2),\n",
    "    #   nn.LeakyReLU(),\n",
    "    #   nn.MaxPool1d(kernel_size=2, stride=6))\n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv1d(16, 32, kernel_size= 7, stride=2, padding=0),\n",
    "      nn.BatchNorm1d(32),\n",
    "      nn.LeakyReLU())\n",
    "    # self.layer3 = nn.Sequential(\n",
    "    #   nn.Conv1d(64, 128, kernel_size= 10, stride = 1, padding = 2),\n",
    "    #   nn.LeakyReLU(),\n",
    "    #   nn.MaxPool1d(kernel_size=2, stride=6))\n",
    "    self.layer3 = nn.Sequential(\n",
    "      nn.Conv1d(32, 32, kernel_size= 7, stride = 2, padding = 0),\n",
    "      nn.BatchNorm1d(32),\n",
    "      nn.LeakyReLU())\n",
    "    # self.layer4 = nn.Sequential(\n",
    "    #   nn.Conv1d(128, 256, kernel_size = 5, stride = 1, padding = 2),\n",
    "    #   nn.ReLU(), \n",
    "    #  nn.MaxPool1d(kernel_size=1, stride=4))\n",
    "    # self.layer5 = nn.Sequential(\n",
    "    #   nn.Conv1d(256, 512, kernel_size = 5, stride = 1, padding = 2),\n",
    "    #   nn.ReLU(), \n",
    "    #   nn.MaxPool1d(kernel_size=1, stride=4))\n",
    "    # self.layer6 = nn.Sequential(\n",
    "    #   nn.Conv1d(512, 1024, kernel_size = 5, stride = 1, padding = 2),\n",
    "    #   nn.ReLU(), \n",
    "    #   nn.MaxPool1d(kernel_size=1, stride=4))\n",
    "\n",
    "    self.layer6_conv1 = nn.Sequential(\n",
    "     nn.Conv1d(2, 1, kernel_size = 1, stride = 1, padding = 0))\n",
    "   \n",
    "\n",
    "    self.layer6 = nn.Sequential(nn.Dropout(p = 0.2))\n",
    "    # self.fc_disease = nn.Linear(1, 1024)\n",
    "    self.fc1 = nn.Linear(1024, 1)\n",
    "\n",
    "    self.extended_tens = torch.zeros((1024, 1))\n",
    "    # self.fc2 = nn.Linear(64, 1)\n",
    "    # self.fc1 = nn.Sequential(nn.Linear(23328, 1))\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x, hie):\n",
    "    out = self.layer1(x)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.layer4(out)\n",
    "    out = self.layer5(out)\n",
    "    out = self.layer6(out)\n",
    "    out_flatten = out.flatten(1)\n",
    "    extended_hie = self.extended_tens(hie, 1024)\n",
    "    out = torch.cat((out_flatten, extended_hie), dim=1)\n",
    "    out = self.layer6_conv1(out)\n",
    "    out = self.fc1(out)\n",
    "    # out = self.sigmoid(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "  def training_step(self, batch_loader, batch_idx):\n",
    "    X, hie, Y = batch_loader\n",
    "    Y_preds = self(X.float(), hie.float())\n",
    "    loss = loss_fn(Y_preds, Y.float())\n",
    "    acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "    self.log('train_loss', loss) \n",
    "    self.log('train_accuracy', acc)    \n",
    "    # self.log('val auc', auc)\n",
    "    return {'loss': loss, 'accuracy':acc} \n",
    "    \n",
    "  def validation_step(self, batch_loader, batch_idx):\n",
    "    X, hie, Y = batch_loader\n",
    "    Y_preds = self(X.float(), hie.float())\n",
    "    loss = loss_fn(Y_preds, Y.float())\n",
    "    acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "    self.log('val_accuracy', acc)\n",
    "    self.log('val_loss', loss)\n",
    "    return {'loss': loss, 'accuracy':acc} \n",
    "  \n",
    "  def test_step(self, batch_loader, batch_idx):\n",
    "    X, hie, Y = batch_loader\n",
    "    Y_preds = self(X.float(), hie.float())\n",
    "    loss = loss_fn(Y_preds, Y.float())\n",
    "    acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "    self.log('test_accuracy', acc)\n",
    "    self.log('test_loss', loss)\n",
    "    return {'loss': loss, 'accuracy':acc} \n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "    return optimizer\n",
    "    # scheduler = StepLR(optimizer , step_size= 100, gamma= 0.1)\n",
    "    # return [optimizer], [scheduler]\n",
    "    \n",
    "    # weight_0 = len(train_label) / (1- sum(train_label) *2)\n",
    "    # weight_1 = len(train_label) / (sum(train_label) *2)\n",
    "    # weight = torch.tensor(sum(train_label)/len(train_label)).cuda()\n",
    "    # loss_fn = nn.BCEWithLogitsLoss(weight = weight)\n",
    "    # class_weights = compute_class_weight('balanced', np.unique(train_label), train_label)\n",
    "    # class_weights_dict = dict(enumerate(class_weights))\n",
    "    # loss_fn = nn.BCELoss()\n",
    "    # loss_fn = nn.BCELoss(weight=weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_sizes[0], padding=(kernel_sizes[0] // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_sizes[1], padding=(kernel_sizes[1] // 2))\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv3 = nn.Conv1d(out_channels, out_channels, kernel_sizes[2], padding=(kernel_sizes[2] // 2))\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.downsample_bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=10, stride=1, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=6)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 64, [10, 10, 10]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=6)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, [10, 10, 10]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=6)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, [5, 5, 5]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=4)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, [5, 5, 5]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(kernel_size=1, stride=4)\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(nn.Dropout(p=0.9))\n",
    "        self.fc_disease = nn.Linear(1, 1024)\n",
    "        self.fc1 = nn.Linear(2048, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hie):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        degree = self.fc_disease(hie)\n",
    "        out = out.flatten(1)\n",
    "        out = torch.cat((out, degree), dim=1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch_loader, batch_idx):\n",
    "        X, hie, Y = batch_loader\n",
    "        Y_preds = self(X.float(), hie.float())\n",
    "        loss = nn.BCELoss(Y_preds, Y.float())\n",
    "        acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return {'loss': loss, 'ACC': acc}\n",
    "\n",
    "    def validation_step(self, batch_loader, batch_idx):\n",
    "        X, hie, Y = batch_loader\n",
    "        Y_preds = self(X.float(), hie.float())\n",
    "        loss = nn.BCELoss(Y_preds, Y.float())\n",
    "        acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        return {'loss': loss, 'ACC': acc}\n",
    "\n",
    "    def test_step(self, batch_loader, batch_idx):\n",
    "        X, hie, Y = batch_loader\n",
    "        Y_preds = self(X.float(), hie.float())\n",
    "        loss = loss_fn(Y_preds, Y.float())\n",
    "        acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "        self.log('test_accuracy', acc)\n",
    "        self.log('test_loss', loss)\n",
    "        return {'loss': loss, 'accuracy':acc} \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    weight_0 = len(train_label) / (1- sum(train_label) *2)\n",
    "    weight_1 = len(train_label) / (sum(train_label) *2)\n",
    "    # weight = torch.tensor(sum(train_label)/len(train_label)).cuda()\n",
    "    # loss_fn = nn.BCEWithLogitsLoss(weight = weight)\n",
    "    # loss_fn = nn.BCELoss(weight=torch.tensor([weight_0, weight_1]))\n",
    "    weight = torch.tensor([[weight_0], [weight_1]])\n",
    "    loss_fn = nn.BCELoss(weight=weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43    47\n",
      "27    29\n",
      "50    55\n",
      "17    19\n",
      "dtype: int64\n",
      "54    59\n",
      "47    51\n",
      "52    57\n",
      "21    23\n",
      "dtype: int64\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | layer1     | Sequential | 704   \n",
      "1 | layer2     | Sequential | 41.0 K\n",
      "2 | layer3     | Sequential | 82.0 K\n",
      "3 | layer4     | Sequential | 164 K \n",
      "4 | layer5     | Sequential | 655 K \n",
      "5 | layer6     | Sequential | 0     \n",
      "6 | fc_disease | Linear     | 2.0 K \n",
      "7 | fc1        | Linear     | 2.0 K \n",
      "------------------------------------------\n",
      "947 K     Trainable params\n",
      "0         Non-trainable params\n",
      "947 K     Total params\n",
      "3.791     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419:  58%|█████▊    | 80/137 [00:02<00:01, 37.67it/s, loss=0.0264, v_num=190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1560/1560 [00:05<00:00, 290.17it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.28205129504203796\n",
      "        test_loss            149.4357147216797\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "3      4\n",
      "9     10\n",
      "55    60\n",
      "19    21\n",
      "dtype: int64\n",
      "49    54\n",
      "35    38\n",
      "25    27\n",
      "13    15\n",
      "dtype: int64\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | layer1     | Sequential | 704   \n",
      "1 | layer2     | Sequential | 41.0 K\n",
      "2 | layer3     | Sequential | 82.0 K\n",
      "3 | layer4     | Sequential | 164 K \n",
      "4 | layer5     | Sequential | 655 K \n",
      "5 | layer6     | Sequential | 0     \n",
      "6 | fc_disease | Linear     | 2.0 K \n",
      "7 | fc1        | Linear     | 2.0 K \n",
      "------------------------------------------\n",
      "947 K     Trainable params\n",
      "0         Non-trainable params\n",
      "947 K     Total params\n",
      "3.791     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/142 [00:00<?, ?it/s, loss=2.29, v_num=191]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 3120/3120 [00:11<00:00, 264.92it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.3461538553237915\n",
      "        test_loss            2.199209213256836\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "#to have k-fold cross validation: \n",
    "\n",
    "for i in range(2):\n",
    "    df = pd.read_csv('nirs with MRI outcome for CNN_10hours_overfree.csv', header = None)\n",
    "    label = pd.read_csv('MRI outcome for CNN_10hours_overfree.csv', header = None)\n",
    "    hie = pd.read_csv('HIE_fixed_for_MRI_overfree.csv', header = None)\n",
    "    train_signal, train_label, train_hie, test_signal, test_label, test_hie, val_signal, val_label, val_hie = making_train_test_val_signals(df, label, hie, 8)\n",
    "    dataset = list(zip(train_signal, train_hie, train_label))\n",
    "    val_dataset = list(zip(val_signal, val_hie, val_label))\n",
    "    test_dataset = list(zip(test_signal, test_hie,  test_label))\n",
    "    #augmentation for training dataset\n",
    "    new_dataset = generate_new_signals_from_one(dataset, 5)\n",
    "    new_signal = np.array([signal for signal, _,_ in new_dataset])\n",
    "    new_signal = new_signal.reshape(len(new_dataset), -1)[ :, :5890]\n",
    "    new_signal = pd.DataFrame(new_signal)\n",
    "    new_signal = new_signal.values\n",
    "    new_signal = new_signal.reshape(-1,1,5890)\n",
    "    new_hie = np.array([hie for _, hie, _ in new_dataset])\n",
    "    new_hie = pd.DataFrame(new_hie)\n",
    "    new_hie = new_hie.values\n",
    "    new_hie = new_hie.reshape(-1,1)\n",
    "    new_label = np.array([label for _, _,label in new_dataset])\n",
    "    new_label = pd.DataFrame(new_label)\n",
    "    new_label = new_label.values\n",
    "    weight = (len(new_label) - sum(new_label))/ sum(new_label)\n",
    "    # weight = 20\n",
    "    print(weight)\n",
    "    new_label = new_label.reshape(-1,1)\n",
    "    newer_dataset = list(zip(new_signal, new_hie, new_label))\n",
    "    #augmentation for validation dataset\n",
    "    new_validation = generate_new_signals_from_one(val_dataset, 5)\n",
    "    new_signal_val = np.array([signal for signal, _,_ in new_validation])\n",
    "    new_signal_val = new_signal_val.reshape(len(new_validation), -1)[ :, :5890]\n",
    "    new_signal_val = pd.DataFrame(new_signal_val)\n",
    "    new_signal_val = new_signal_val.values\n",
    "    new_signal_val = new_signal_val.reshape(-1,1,5890)\n",
    "    new_hie_val = np.array([hie for _, hie, _ in new_validation])\n",
    "    new_hie_val = pd.DataFrame(new_hie_val)\n",
    "    new_hie_val = new_hie_val.values\n",
    "    new_hie_val = new_hie_val.reshape(-1,1)\n",
    "    new_label_val = np.array([label for _, _,label in new_validation])\n",
    "    new_label_val = pd.DataFrame(new_label_val)\n",
    "    new_label_val = new_label_val.values\n",
    "    new_label_val = new_label_val.reshape(-1,1)\n",
    "    newer_validation = list(zip(new_signal_val, new_hie_val, new_label_val))\n",
    "    #augmentation for test dataset\n",
    "    new_test = generate_new_signals_from_one(test_dataset, 5)\n",
    "    new_signal_test = np.array([signal for signal, _,_ in new_test])\n",
    "    new_signal_test = new_signal_test.reshape(len(new_test), -1)[ :, :5890]\n",
    "    new_signal_test = pd.DataFrame(new_signal_test)\n",
    "    new_signal_test = new_signal_test.values\n",
    "    new_signal_test = new_signal_test.reshape(-1,1,5890)\n",
    "    new_hie_test = np.array([hie for _, hie, _ in new_test])\n",
    "    new_hie_test = pd.DataFrame(new_hie_test)\n",
    "    new_hie_test = new_hie_test.values\n",
    "    new_hie_test = new_hie_test.reshape(-1,1)\n",
    "    new_label_test = np.array([label for _, _,label in new_test])\n",
    "    new_label_test = pd.DataFrame(new_label_test)\n",
    "    new_label_test = new_label_test.values\n",
    "    new_label_test = new_label_test.reshape(-1,1)\n",
    "    newer_test = list(zip(new_signal_test, new_hie_test, new_label_test))\n",
    "    # new_hie.to_csv('fromone.csv', header=None, index=False)\n",
    "    # training_dataset = pd.read_csv('fromone.csv', header = None)\n",
    "    # for i in range(10):\n",
    "    #     if i <4:\n",
    "    #         num = 1\n",
    "    #         filename = f'combined_signals_train_label_1_{i}.csv'\n",
    "    #         df_combined_signals = generate_new_signals(dataset,100, 1000, 589, 1)\n",
    "    #         df_combined_signals = pd.DataFrame(df_combined_signals)\n",
    "    #         df_combined_signals.to_csv(filename, header=None, index=False)\n",
    "    #     else:\n",
    "    #         num = 0\n",
    "    #         filename = f'combined_signals_train_label_0_{i-4}.csv'\n",
    "    #         df_combined_signals = generate_new_signals(dataset,100, 1000, 589, 0)\n",
    "    #         df_combined_signals = pd.DataFrame(df_combined_signals)\n",
    "    #         df_combined_signals.to_csv(filename, header=None, index=False)\n",
    "\n",
    "    # df_00 = pd.read_csv('combined_signals_train_label_0_0.csv', header = None)\n",
    "    # df_01 = pd.read_csv('combined_signals_train_label_0_1.csv', header = None)\n",
    "    # df_02 = pd.read_csv('combined_signals_train_label_0_2.csv', header = None)\n",
    "    # df_03 = pd.read_csv('combined_signals_train_label_0_3.csv', header = None)\n",
    "    # df_04 = pd.read_csv('combined_signals_train_label_0_4.csv', header = None)\n",
    "    # df_05 = pd.read_csv('combined_signals_train_label_0_5.csv', header = None)\n",
    "    # df_10 = pd.read_csv('combined_signals_train_label_1_0.csv', header = None)\n",
    "    # df_11 = pd.read_csv('combined_signals_train_label_1_1.csv', header = None)\n",
    "    # df_12 = pd.read_csv('combined_signals_train_label_1_2.csv', header = None)\n",
    "    # df_13 = pd.read_csv('combined_signals_train_label_1_3.csv', header = None)\n",
    "    # combined_signals = np.concatenate([train_signal, df_00, df_01, df_02, df_03,\n",
    "    #                                df_04, df_05, df_10, df_11, df_12, df_13], axis=0)\n",
    "    # arr0 = np.zeros((6000,1))\n",
    "    # arr1 = np.ones((4000,1))\n",
    "    # train_label = np.array(train_label)\n",
    "    # train_label = train_label.reshape(-1,1)\n",
    "    # combined_label = np.concatenate([train_label, arr0, arr1], axis = 0)\n",
    "    # combined_label = combined_label.reshape(-1,1)\n",
    "    # train_hie = np.array(train_hie)\n",
    "    # train_hie = train_hie.reshape(-1,1)\n",
    "    # combined_hie = np.concatenate([train_hie, arr0, arr1], axis= 0)\n",
    "    # combined_hie = combined_hie.reshape(-1,1)\n",
    "    # combined_signals = combined_signals.reshape(-1,1,5890)\n",
    "    # combined_signals = torch.tensor(combined_signals)\n",
    "    # combined_label = torch.tensor(combined_label)\n",
    "    # combined_hie = torch.tensor(combined_hie)\n",
    "    # dataset = list(zip(combined_signals, combined_hie, combined_label))\n",
    "    \n",
    "    positive_weight = torch.ones([1])* weight\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight = positive_weight.cuda())\n",
    "    # loss_fn = torch.nn.BCELoss(weight=torch.tensor([10000.,1.]), reduction='sum')\n",
    "    train_loader = DataLoader(newer_dataset, batch_size= 256, shuffle=True)\n",
    "    val_loader = DataLoader(newer_validation, batch_size= 64)\n",
    "    test_loader = DataLoader(newer_test)\n",
    "    model = CNN()\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\", devices= 1, max_epochs= 1000)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28800, 1)\n"
     ]
    }
   ],
   "source": [
    "print(new_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#to see if the vsc is using cpu or gpu \n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6135, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_hat = model(torch.tensor(new_signal_test).float(), torch.tensor(new_hie_test).float())\n",
    "auc = torch.tensor(roc_auc_score(new_label_test, y_hat.detach()))\n",
    "print(auc)\n",
    "prediction = (y_hat >= 0.5).int()\n",
    "prediction = pd.DataFrame(y_hat)\n",
    "prediction.to_csv(\"prediciton.csv\", header=None, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11528), started 10 days, 1:05:08 ago. (Use '!kill 11528' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e82ce439d8e3e4b8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e82ce439d8e3e4b8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "#for seeing the graphs, first cnt+shift+p; then search for \"Python: Launch TensorBoard\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f016566b22cad909ae7371bfe6dcf982c46e7e0221c8e02452fe5989747123"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
