{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\120224294\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from torchmetrics.functional import accuracy\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import ConcatDataset\n",
    "import random\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.metrics import roc_auc_score, f1_score, matthews_corrcoef\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_test_val_ids(df, n):\n",
    "    IDs = df[0].unique()\n",
    "    random_ids = pd.Series(IDs).sample(n , replace= False)\n",
    "    return random_ids\n",
    "\n",
    "def making_train_test_val_signals(df, label, n):\n",
    "    a = making_test_val_ids(df, n)\n",
    "    train_signal = df[~df[0].isin(a)]\n",
    "    train_label = label[~label[0].isin(a)]\n",
    "    test_IDs = a[:(n//2)]\n",
    "    val_IDs = a[(n//2):]\n",
    "    train_signal = train_signal.values\n",
    "    train_signal = train_signal[:,1:]\n",
    "    # train_hie = hie[~hie[0].isin(a)]\n",
    "    # train_hie = train_hie.values\n",
    "    # train_hie = train_hie[:,2]\n",
    "    train_label = train_label.values\n",
    "    train_label = train_label[:,1]\n",
    "    test_signal = df[df[0].isin(test_IDs)]\n",
    "    test_signal = test_signal.values\n",
    "    test_signal = test_signal[:,1:]\n",
    "    test_label = label[label[0].isin(test_IDs)]\n",
    "    test_label = test_label.values\n",
    "    test_label = test_label[:,1]\n",
    "    # test_hie = hie[hie[0].isin(test_IDs)]\n",
    "    # test_hie = test_hie.values\n",
    "    # test_hie = test_hie[:,2]\n",
    "    val_signal = df[df[0].isin(val_IDs)]\n",
    "    val_signal = val_signal.values\n",
    "    val_signal = val_signal[:,1:]\n",
    "    val_label = label[label[0].isin(val_IDs)]\n",
    "    val_label = val_label.values\n",
    "    val_label = val_label[:,1]\n",
    "    # val_hie = hie[hie[0].isin(val_IDs)]\n",
    "    # val_hie = val_hie.values\n",
    "    # val_hie = val_hie[:,2]\n",
    "    return train_signal, train_label, test_signal, test_label, val_signal, val_label\n",
    "\n",
    "def separate_epochs(train_signal, epoch_length):\n",
    "    num_epochs = 5890 // epoch_length\n",
    "    separated_epochs = np.split(train_signal, num_epochs, axis=1)\n",
    "    return separated_epochs\n",
    "\n",
    "def shuffle_epochs(epochs):\n",
    "    np.random.shuffle(epochs)\n",
    "    return epochs\n",
    "\n",
    "def reconstruct_signals(epochs):\n",
    "    reconstructed_signals = np.concatenate(epochs, axis=1)\n",
    "    return reconstructed_signals\n",
    "\n",
    "def separate_pieces(signal, num_pieces):\n",
    "    piece_length = 5890 // num_pieces\n",
    "    pieces = np.split(signal, num_pieces, axis=1)\n",
    "    return pieces\n",
    "\n",
    "\n",
    "\n",
    "def generate_new_signals(dataset, num_signals, num_rep, epoch_length, num):\n",
    "    filtered_dataset = [(train_signal, train_hie, train_label) for train_signal, train_hie,  train_label in dataset if train_label == num and train_hie == num]\n",
    "    filtered_signals = np.array([train_signal for train_signal, _ , _ in filtered_dataset])\n",
    "    filtered_signals = filtered_signals.reshape(-1, 1, 5890)\n",
    "    combined_signals = []\n",
    "    for _ in range(num_rep):\n",
    "        selected_indices = np.random.choice(filtered_signals.shape[0], size=num_signals, replace=False)\n",
    "        selected_signals = filtered_signals[selected_indices]\n",
    "\n",
    "        combined_epochs = []\n",
    "        for train_signal in selected_signals:\n",
    "            epochs = separate_epochs(train_signal, epoch_length)\n",
    "            combined_epochs += epochs\n",
    "\n",
    "        shuffled_epochs = shuffle_epochs(combined_epochs)\n",
    "        reconstructed_signals = reconstruct_signals(shuffled_epochs)\n",
    "        combined_signals.append(reconstructed_signals)\n",
    "\n",
    "    combined_signals = np.array(combined_signals)\n",
    "    # combined_signals = combined_signals.reshape(num_rep, 1, 5890)\n",
    "    combined_signals = combined_signals.reshape(num_rep, -1)[ :, :5890]\n",
    "    return combined_signals\n",
    "\n",
    "def generate_new_signals_from_one(dataset, num_pieces):\n",
    "    new_signals = []\n",
    "    for signal, label in dataset:\n",
    "        signal = signal.reshape(1, 5890)\n",
    "        pieces = separate_pieces(signal, num_pieces)\n",
    "        permutations = list(itertools.permutations(pieces))\n",
    "        for perm in permutations:\n",
    "            # shuffled_pieces = shuffle_epochs(perm)\n",
    "            new_signal = np.concatenate(perm, axis=1)\n",
    "            new_signals.append((new_signal, label))\n",
    "    return new_signals\n",
    "\n",
    "\n",
    "def generate_new_rows_amin(row, num_new_rows, window_percentage):\n",
    "    new_rows = []\n",
    "    for _ in range(num_new_rows):\n",
    "        row_length = len(row)\n",
    "        window_size = int(row_length * window_percentage)\n",
    "\n",
    "        # Randomly select start indices for the windows\n",
    "        start_idx_orig = random.randint(1, row_length - window_size)\n",
    "        start_idx_swap = random.randint(1, row_length - window_size)\n",
    "\n",
    "        # Generate the new row by swapping the windows\n",
    "        new_row = row[:]\n",
    "        new_row[start_idx_orig:start_idx_orig + window_size], new_row[start_idx_swap:start_idx_swap + window_size] = \\\n",
    "            new_row[start_idx_swap:start_idx_swap + window_size], new_row[start_idx_orig:start_idx_orig + window_size]\n",
    "\n",
    "        new_rows.append(new_row)\n",
    "    return new_rows\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(pl.LightningModule):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.layer1 = nn.Sequential(\n",
    "      nn.Conv1d(1, 8, kernel_size= 8, stride=2, padding=0),\n",
    "      nn.BatchNorm1d(8),\n",
    "      nn.ReLU())\n",
    "    \n",
    "    self.layer2 = nn.Sequential(\n",
    "      nn.Conv1d(8, 16, kernel_size= 10, stride=2, padding=0),\n",
    "      nn.BatchNorm1d(16),\n",
    "      nn.ReLU())\n",
    "    \n",
    "    self.layer3 = nn.Sequential(\n",
    "      nn.Conv1d(16, 32, kernel_size= 10, stride = 2, padding = 0),\n",
    "      nn.ReLU())\n",
    "    \n",
    "    self.layer4 = nn.Sequential(nn.Dropout(p = 0.9))\n",
    "    self.fc1 = nn.Linear(23328 , 1)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.layer1(x)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.layer4(out)\n",
    "    out = out.flatten(1)\n",
    "    out = self.fc1(out)\n",
    "    #x = self.sigmoid(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "  def training_step(self, batch_loader, batch_idx):\n",
    "    X, Y = batch_loader\n",
    "    Y_preds = self(X.float())\n",
    "    loss = loss_fn(Y_preds, Y.float())\n",
    "    acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "    # auc = torch.tensor(roc_auc_score(Y.cpu(), Y_preds.detach().cpu()))\n",
    "    self.log('train_loss', loss) \n",
    "    self.log('train_accuracy', acc)    \n",
    "    # self.log('val auc', auc)\n",
    "    return {'loss': loss, 'accuracy':acc} \n",
    "    \n",
    "  def validation_step(self, batch_loader, batch_idx):\n",
    "    X, Y = batch_loader\n",
    "    Y_preds = self(X.float())\n",
    "    loss = loss_fn(Y_preds, Y.float())\n",
    "    acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "    # auc = torch.tensor(roc_auc_score(Y.cpu(), Y_preds.detach().cpu()))\n",
    "    self.log('val_accuracy', acc)\n",
    "    self.log('val_loss', loss)\n",
    "    # self.log('val auc', auc)\n",
    "    return {'loss': loss, 'accuracy':acc} \n",
    "  \n",
    "  def test_step(self, batch_loader, batch_idx):\n",
    "    X, Y = batch_loader\n",
    "    Y_preds = self(X.float())\n",
    "    loss = loss_fn(Y_preds, Y.float())\n",
    "    acc = accuracy(Y_preds, Y, task = \"binary\")\n",
    "    # auc = torch.tensor(roc_auc_score(Y.cpu(), Y_preds.detach().cpu()))\n",
    "    # threshold = 0.2  # Adjust this threshold as per your requirement\n",
    "    # Y_pred_binary = (Y_preds >= threshold).float()\n",
    "    # Y = Y.cpu().numpy()\n",
    "    # Y_pred_binary = Y_pred_binary.detach().cpu().numpy()\n",
    "    # # Calculate F1 score\n",
    "    # f1 = torch.tensor(f1_score(Y, Y_pred_binary, zero_division=1))\n",
    "    # Calculate Matthews correlation coefficient (MCC)\n",
    "    # mcc = torch.tensor(matthews_corrcoef(Y, Y_pred_binary))\n",
    "    # f1 = f1_score(Y.cpu(), Y_preds.detach().cpu())\n",
    "    # mcc = matthews_corrcoef(Y.cpu(), Y_preds.detach().cpu())\n",
    "    self.log('test_accuracy', acc)\n",
    "    self.log('test_loss', loss)\n",
    "    # self.log('test_f1',f1)\n",
    "    # self.log('test_auc', auc)\n",
    "    return {'loss': loss, 'accuracy':acc} \n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr= 0.00001)\n",
    "    return optimizer\n",
    "    \n",
    "    \n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model with HIE labels\n",
    "df = pd.read_csv('nirs with MRI outcome for CNN_10hours_overfree.csv', header = None)\n",
    "label = pd.read_csv('HIE_fixed_for_MRI_overfree.csv', header = None)\n",
    "df = df.values\n",
    "df = df[:,1:]\n",
    "label = label.values\n",
    "label = label[:,1]\n",
    "dataset = list(zip(df, label))\n",
    "new_dataset = generate_new_signals_from_one(dataset, 5)\n",
    "new_signal = np.array([signal for signal, _ in new_dataset])\n",
    "new_signal = new_signal.reshape(len(new_dataset), -1)[ :, :5890]\n",
    "new_signal = pd.DataFrame(new_signal)\n",
    "new_signal = new_signal.values\n",
    "new_signal = new_signal.reshape(-1,1,5890)\n",
    "new_label = np.array([label for _ ,label in new_dataset])\n",
    "new_label = pd.DataFrame(new_label)\n",
    "new_label = new_label.values\n",
    "new_label = new_label.reshape(-1,1)\n",
    "newer_dataset = list(zip(new_signal, new_label))\n",
    "train_loader = DataLoader(newer_dataset, batch_size=64, shuffle=True)\n",
    "model = CNN()\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices= 1, max_epochs= 2000, default_root_dir=\"model\")\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model = model.load_from_checkpoint(r\"model\\lightning_logs\\version_15\\checkpoints\\epoch=1999-step=1070000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc1\" in name or \"layer4\" in name:\n",
    "        param.requires_grad = True\n",
    "    else: \n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    df = pd.read_csv('nirs with MRI outcome for CNN_10hours_overfree.csv', header = None)\n",
    "    label = pd.read_csv('MRI outcome for CNN_10hours_overfree.csv', header = None)\n",
    "    train_signal, train_label, test_signal, test_label, val_signal, val_label = making_train_test_val_signals(df, label, 8)\n",
    "    val_label = val_label.reshape(-1,1)\n",
    "    val_signal = val_signal.reshape(-1,1,5890)\n",
    "    test_label = test_label.reshape(-1,1)\n",
    "    test_signal = test_signal.reshape(-1,1,5890)\n",
    "    train_label = train_label.reshape(-1,1)\n",
    "    train_signal = train_signal.reshape(-1,1,5890)\n",
    "    dataset = list(zip(train_signal, train_label))\n",
    "    val_dataset = list(zip(val_signal, val_label))\n",
    "    test_dataset = list(zip(test_signal,  test_label))\n",
    "    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size= 64)\n",
    "    test_loader = DataLoader(test_dataset)\n",
    "    trainer = pl.Trainer( max_epochs= 1, log_every_n_steps=1,gpus=1, default_root_dir=\"model\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16272), started 129 days, 22:15:22 ago. (Use '!kill 16272' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9ff82b2793f97f83\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9ff82b2793f97f83\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
